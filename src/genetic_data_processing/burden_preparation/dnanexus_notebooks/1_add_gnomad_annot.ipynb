{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import hail as hl\n",
    "import os\n",
    "import time\n",
    "import dxpy\n",
    "import logging\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "\n",
    "# Had to set the configuration to navigate RDD partition error\n",
    "# Build spark\n",
    "builder = (\n",
    "    SparkSession\n",
    "    .builder\n",
    "    .appName(\"gnomad annotation\")  # Set a meaningful application name\n",
    "    .config(\"spark.driver.memory\", \"12g\")  # Set driver memory (e.g., 8 GB)\n",
    "    .config(\"spark.executor.memory\", \"12g\")  # Set executor memory (e.g., 16 GB)\n",
    "    .config(\"spark.executor.cores\", \"14\")  # Optional: Set number of cores per executor \n",
    "    .enableHiveSupport()\n",
    ")\n",
    "spark = builder.getOrCreate()\n",
    "\n",
    "hl.init(sc=spark.sparkContext, idempotent=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def save_in_hail_format(hail_obj, db_name, hail_obj_name, rerun):\n",
    "    # Create DB if it does not exist\n",
    "    stmt = f\"CREATE DATABASE IF NOT EXISTS {db_name} LOCATION 'dnax://'\"\n",
    "    spark.sql(stmt).show()\n",
    "    # Find database ID of newly created database using dxpy method\n",
    "    db_uri = dxpy.find_one_data_object(name=f\"{db_name}\".lower(), classname=\"database\")['id']\n",
    "    # Write hail object\n",
    "    url = f\"dnax://{db_uri}/{hail_obj_name}\"\n",
    "    if rerun:\n",
    "        hail_obj.write(url, overwrite=True)\n",
    "    return url\n",
    "\n",
    "def get_url(db_name, hail_obj_name):\n",
    "    # Find database ID of newly created database using dxpy method\n",
    "    db_uri = dxpy.find_one_data_object(name=f\"{db_name}\".lower(), classname=\"database\")['id']\n",
    "    # Write hail object\n",
    "    url = f\"dnax://{db_uri}/{hail_obj_name}\"\n",
    "    return url\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "RERUN_VAT=False\n",
    "RERUN_VEP=False\n",
    "RERUN_GNOMAD=False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save locus table in hail format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def process_locus_alleles(ht):\n",
    "    ht = ht.annotate(\n",
    "        locus=hl.locus(ht.locus.split(\":\")[0], hl.int(ht.locus.split(\":\")[1]), reference_genome='GRCh38'),\n",
    "        alleles=ht.alleles.split(\"_\")\n",
    "    )\n",
    "    ht = ht.key_by(\"locus\", \"alleles\")\n",
    "    return ht"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "if RERUN_VAT:\n",
    "    variant_file = \"file:///mnt/project/notebooks/wes/burden_preparation/data/ukb_ptv_locus.tsv\"\n",
    "    variant_ht = hl.import_table(variant_file)\n",
    "    variant_ht = process_locus_alleles(variant_ht)\n",
    "    variant_ht = variant_ht.repartition(100)\n",
    "    url = save_in_hail_format(variant_ht, \"variant_annot\", \"ukb_ptv_locus.ht\", rerun=RERUN_VAT)\n",
    "    variant_ht = hl.read_table(url)\n",
    "else:\n",
    "    url = get_url(\"variant_annot\", \"ukb_ptv_locus.ht\")\n",
    "    variant_ht = hl.read_table(url)\n",
    "    print(variant_ht.count())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Annotate with vep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def add_vep_annotations(ht, vep_file=\"file:///mnt/project/notebooks/wes/burden_preparation/data/vep_config_109_v8.json\"):\n",
    "    \"\"\"\n",
    "    Add vep annotations\n",
    "    \"\"\"\n",
    "    # add vep annotations\n",
    "    ht = hl.vep(ht, vep_file) # annot table with vep\n",
    "    return ht"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "if RERUN_VEP:\n",
    "    variant_ht = add_vep_annotations(variant_ht)\n",
    "    url = save_in_hail_format(variant_ht, \"variant_annot\", \"ukb_ptv_locus_annot.ht\", rerun=RERUN_VEP)\n",
    "    variant_ht = hl.read_table(url)\n",
    "else:\n",
    "    url = get_url(\"variant_annot\", \"ukb_ptv_locus_annot.ht\")\n",
    "    variant_ht = hl.read_table(url)\n",
    "    print(variant_ht.count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process and filter gnomad columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def add_gnomad_annotations(ht):\n",
    "    # create dummy struct for missing variants in gnomad\n",
    "    dummy_struct = hl.struct(\n",
    "        gnomade=0.0, gnomade_afr=0.0, gnomade_amr=0.0, gnomade_eas=0.0, \n",
    "        gnomade_fin=0.0, gnomade_nfe=0.0, gnomade_sas=0.0\n",
    "    )\n",
    "    # Replace all the missing variants in gnomad with array of dict of struct\n",
    "    ht = ht.annotate(\n",
    "        gnomad_freq=hl.or_else(\n",
    "            ht.vep.colocated_variants.frequencies, \n",
    "            hl.array([{\"NA\": dummy_struct}]))\n",
    "    )\n",
    "    ht = ht.explode(ht.gnomad_freq)\n",
    "    ht = ht.annotate(\n",
    "        gnomade=ht.gnomad_freq.get(ht.alleles[1]).gnomade,\n",
    "        gnomade_afr=ht.gnomad_freq.get(ht.alleles[1]).gnomade_afr,\n",
    "        gnomade_amr=ht.gnomad_freq.get(ht.alleles[1]).gnomade_amr,\n",
    "        gnomade_eas=ht.gnomad_freq.get(ht.alleles[1]).gnomade_eas,\n",
    "        gnomade_fin=ht.gnomad_freq.get(ht.alleles[1]).gnomade_fin,\n",
    "        gnomade_nfe=ht.gnomad_freq.get(ht.alleles[1]).gnomade_nfe,\n",
    "        gnomade_sas=ht.gnomad_freq.get(ht.alleles[1]).gnomade_sas,\n",
    "    )\n",
    "    ht = ht.select(ht.gnomade, ht.gnomade_afr, ht.gnomade_amr, ht.gnomade_eas, ht.gnomade_sas, gnomade_eur=hl.max(ht.gnomade_fin, ht.gnomade_nfe))\n",
    "    return ht\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "if RERUN_GNOMAD:\n",
    "    variant_ht = add_gnomad_annotations(variant_ht)\n",
    "    url = save_in_hail_format(variant_ht, \"variant_annot\", \"ukb_ptv_gnomad_annot.ht\", rerun=RERUN_GNOMAD)\n",
    "    variant_ht = hl.read_table(url)\n",
    "else:\n",
    "    url = get_url(\"variant_annot\", \"ukb_ptv_gnomad_annot.ht\")\n",
    "    variant_ht = hl.read_table(url)\n",
    "    print(variant_ht.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save as pandas df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def upload_file_to_project(filename, proj_dir):\n",
    "    dxpy.upload_local_file(filename, folder=proj_dir, parents=True)\n",
    "    print(f\"*********{filename} uploaded!!*********\")\n",
    "    os.remove(filename)\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "gnomad_df = variant_ht.to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get gnomad pop max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "gnomad_df = gnomad_df.fillna(0)\n",
    "gnomad_df[\"locus\"] = gnomad_df.locus.astype(str)\n",
    "gnomad_df[\"alleles\"] = gnomad_df.alleles.apply(lambda x: \"_\".join(x))\n",
    "gnomad_df = gnomad_df.groupby([\"locus\", \"alleles\"]).agg(max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "gnomad_df[\"maf_gnomad_popmax\"] = gnomad_df.loc[:, [\"gnomade_afr\", \"gnomade_amr\", \"gnomade_eas\", \"gnomade_sas\", \"gnomade_eur\"]].max(axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "proj_dir = f\"/notebooks/wes/burden_preparation/data/\"\n",
    "filename = f\"gnomad_annot.tsv.gz\"\n",
    "gnomad_df.to_csv(filename, sep='\\t', index=True)\n",
    "upload_file_to_project(filename, proj_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
