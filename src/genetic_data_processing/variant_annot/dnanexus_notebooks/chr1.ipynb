{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import hail as hl\n",
    "import os\n",
    "import time\n",
    "import dxpy\n",
    "import logging\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "\n",
    "# Had to set the configuration to navigate RDD partition error\n",
    "# Build spark\n",
    "builder = (\n",
    "    SparkSession\n",
    "    .builder\n",
    "    .appName(\"Variant annotation\")  # Set a meaningful application name\n",
    "    .config(\"spark.driver.memory\", \"96g\")  # Set driver memory (e.g., 8 GB)\n",
    "    .config(\"spark.executor.memory\", \"108g\")  # Set executor memory (e.g., 16 GB)\n",
    "    .config(\"spark.executor.cores\", \"30\")  # Optional: Set number of cores per executor \n",
    "    .enableHiveSupport()\n",
    ")\n",
    "spark = builder.getOrCreate()\n",
    "\n",
    "hl.init(sc=spark.sparkContext, idempotent=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def save_in_hail_format(hail_obj, db_name, hail_obj_name, rerun):\n",
    "    # Create DB if it does not exist\n",
    "    stmt = f\"CREATE DATABASE IF NOT EXISTS {db_name} LOCATION 'dnax://'\"\n",
    "    spark.sql(stmt).show()\n",
    "    # Find database ID of newly created database using dxpy method\n",
    "    db_uri = dxpy.find_one_data_object(name=f\"{db_name}\".lower(), classname=\"database\")['id']\n",
    "    # Write hail object\n",
    "    url = f\"dnax://{db_uri}/{hail_obj_name}\"\n",
    "    if rerun:\n",
    "        hail_obj.write(url, overwrite=True)\n",
    "    return url\n",
    "\n",
    "def get_url(db_name, hail_obj_name):\n",
    "    # Find database ID of newly created database using dxpy method\n",
    "    db_uri = dxpy.find_one_data_object(name=f\"{db_name}\".lower(), classname=\"database\")['id']\n",
    "    # Write hail object\n",
    "    url = f\"dnax://{db_uri}/{hail_obj_name}\"\n",
    "    return url\n",
    "\n",
    "def get_chrm_mt(chr_num):\n",
    "    db_name = f\"exomes\"\n",
    "    # Find database ID of newly created database using dxpy method\n",
    "    db_uri = dxpy.find_one_data_object(name=f\"{db_name}\".lower(), classname=\"database\")['id']\n",
    "    url = f\"dnax://{db_uri}/chr{chr_num}_vqc.mt\"\n",
    "    mt = hl.read_matrix_table(url)\n",
    "    return mt\n",
    "\n",
    "def upload_file_to_project(filename, proj_dir):\n",
    "    dxpy.upload_local_file(filename, folder=proj_dir, parents=True)\n",
    "    print(f\"*********{filename} uploaded!!*********\")\n",
    "    os.remove(filename)\n",
    "    return\n",
    "\n",
    "def get_rare_variants(mt):\n",
    "    \"\"\"\n",
    "    Returns a matrix table with alt allele frequency < 0.01\n",
    "    \"\"\"\n",
    "    # with the filtered variants calculate \n",
    "    mt = mt.annotate_rows(gt_stats = hl.agg.call_stats(mt.GT, mt.alleles))\n",
    "    # filter to keep rare (1%) variants and variants present in at least one sample\n",
    "    mt = mt.filter_rows((mt.gt_stats.AF[1] < 0.01) & (mt.gt_stats.AC[1] > 0))\n",
    "    # add maf and mac info\n",
    "    mt = mt.annotate_rows(maf=mt.gt_stats.AF[1], mac=mt.gt_stats.AC[1])\n",
    "    return mt\n",
    "\n",
    "def variant_qc(mt):\n",
    "    # annotate variant call rate, hwe p-value, min read depth\n",
    "    mt = mt.annotate_rows(\n",
    "        call_rate=mt.variant_qc.call_rate,\n",
    "        p_value_hwe=mt.variant_qc.p_value_hwe,\n",
    "        min_rd=mt.variant_qc.dp_stats.min,\n",
    "    )\n",
    "    return mt\n",
    "\n",
    "def sample_qc(mt, sample_qc_annot_file=\"file:///mnt/project/notebooks/wes/sample_qc/data/flagged_samples.tsv\"):\n",
    "    sample_annot_ht = hl.import_table(sample_qc_annot_file)\n",
    "    sample_annot_ht = sample_annot_ht.key_by('s')\n",
    "    mt = mt.annotate_cols(sample_filters=sample_annot_ht[mt.s].filters)\n",
    "    mt = mt.filter_cols(mt.sample_filters==\"\")\n",
    "    return mt\n",
    "\n",
    "def add_vep_annotations(mt, vep_file=\"file:///mnt/project/notebooks/wes/variant_annot/data/vep_config_109_v7.json\"):\n",
    "    \"\"\"\n",
    "    Add vep and dbnsfp annotations\n",
    "    \"\"\"\n",
    "    # add vep annotations\n",
    "    mt = hl.vep(mt, vep_file) # annot table with vep\n",
    "    # combine multiple consequences for a single transcript into one string\n",
    "    mt = mt.annotate_rows(consequences = hl.map(lambda x: hl.delimit(x, delimiter=\";\"), mt.vep.transcript_consequences.consequence_terms))\n",
    "    # annotate genes, transcripts, consequences and biotype\n",
    "    mt = mt.annotate_rows(gene_transcript_consequence_biotype = hl.zip(\n",
    "        mt.vep.transcript_consequences.gene_symbol,\n",
    "        mt.vep.transcript_consequences.transcript_id,\n",
    "        mt.consequences,\n",
    "        mt.vep.transcript_consequences.biotype,\n",
    "        mt.vep.transcript_consequences.lof,\n",
    "    ))\n",
    "    # only keep relevant columns\n",
    "    mt = mt.select_rows(\n",
    "        mt.gene_transcript_consequence_biotype,\n",
    "        mt.maf, mt.mac,\n",
    "        mt.call_rate, mt.p_value_hwe, mt.min_rd\n",
    "    )\n",
    "    # explode by gene-trancsript-consequence column\n",
    "    mt = mt.explode_rows(\"gene_transcript_consequence_biotype\")\n",
    "    \n",
    "    # get plof and missense mutations\n",
    "    lof_mutations = \"stop_gained|frameshift_variant|stop_lost|start_lost\"\n",
    "    splice_lof_mutations = \"splice_acceptor_variant|splice_donor_variant\"\n",
    "    missense_mutations = \"missense_variant\"\n",
    "    \n",
    "    mt = mt.annotate_rows(\n",
    "        lof = mt.gene_transcript_consequence_biotype[2].matches(lof_mutations),\n",
    "        missense = mt.gene_transcript_consequence_biotype[2].matches(missense_mutations),\n",
    "        splice_lof = mt.gene_transcript_consequence_biotype[2].matches(splice_lof_mutations)\n",
    "    )\n",
    "    \n",
    "    # filter for these mutation types\n",
    "    mt = mt.filter_rows((mt.lof==True)|(mt.splice_lof==True)|(mt.missense==True))\n",
    "    return mt\n",
    "\n",
    "def create_deleteriousness_scores(mt):\n",
    "    metrics = [\"SIFT\", \"LRT\", \"FATHMM\", \"PROVEAN\", \"MetaSVM\", \"MetaLR\", \"PrimateAI\", \"DEOGEN2\", \"MutationAssessor\"]\n",
    "    kwd_dict = {f\"{m}_pred\": hl.dict(hl.zip(\n",
    "            mt.dbNSFP_variants.genename,\n",
    "            hl.map(lambda x: hl.dict(hl.zip(x[0].split(\";\"), x[1].split(\";\"))), hl.zip(mt.dbNSFP_variants.Ensembl_transcriptid, mt.dbNSFP_variants[f\"{m}_pred\"]))\n",
    "    )) for m in metrics}\n",
    "    mt = mt.annotate_rows(**kwd_dict)\n",
    "    \n",
    "    def get_del_score_func(gtcb, del_pred):\n",
    "        gene = gtcb[0]\n",
    "        transcript = gtcb[1]\n",
    "        val = hl.if_else(del_pred.contains(gene) & del_pred[gene].contains(transcript) & (del_pred[gene][transcript]==\"D\"), 1, 0)\n",
    "        return val\n",
    "    \n",
    "    kwd_dict = {f\"{m}_pred\": get_del_score_func(mt.gene_transcript_consequence_biotype, mt[f\"{m}_pred\"]) for m in metrics[:-1]}\n",
    "    mt = mt.annotate_rows(**kwd_dict)\n",
    "\n",
    "    def get_del_score_func_ma(gtcb, del_pred):\n",
    "        gene = gtcb[0]\n",
    "        transcript = gtcb[1]\n",
    "        val = hl.if_else(del_pred.contains(gene) & del_pred[gene].contains(transcript) & (del_pred[gene][transcript]==\"H\"), 1, 0)\n",
    "        return val\n",
    "    \n",
    "    mt = mt.annotate_rows(MutationAssessor_pred = get_del_score_func_ma(mt.gene_transcript_consequence_biotype, mt.MutationAssessor_pred))\n",
    "    cols2sum = [f\"{m}_pred\" for m in metrics]\n",
    "    mt = mt.annotate_rows(del_score=hl.sum([mt[col] for col in cols2sum]))\n",
    "    return mt\n",
    "    \n",
    "def add_dbnsfp_annotations(mt):\n",
    "    db = hl.experimental.DB(region='us', cloud='aws')\n",
    "    mt = db.annotate_rows_db(mt, 'dbNSFP_variants') # add dbNSFP annotations\n",
    "    mt = create_deleteriousness_scores(mt)\n",
    "    return mt\n",
    "\n",
    "def keep_deleterious_variants(mt):\n",
    "    # filter to keep deleterious mutations only\n",
    "    mt = mt.filter_rows((mt.lof==True)|(mt.splice_lof==True)|((mt.missense==True)&(mt.del_score>4)))\n",
    "    # annotate properly\n",
    "    mt = mt.annotate_rows(\n",
    "        gene = mt.gene_transcript_consequence_biotype[0],\n",
    "        transcript = mt.gene_transcript_consequence_biotype[1],\n",
    "        consequence = mt.gene_transcript_consequence_biotype[2],\n",
    "        biotype = mt.gene_transcript_consequence_biotype[3],\n",
    "        loftee = mt.gene_transcript_consequence_biotype[4]\n",
    "    )\n",
    "    # only keep relevant row information\n",
    "    mt  = mt.select_rows(\n",
    "        mt.gene, mt.transcript, mt.consequence, mt.biotype, mt.loftee,\n",
    "        mt.lof, mt.splice_lof, mt.missense, mt.del_score,\n",
    "        mt.maf, mt.mac, mt.call_rate, mt.p_value_hwe, mt.min_rd\n",
    "    )\n",
    "    return mt\n",
    "\n",
    "def add_sample_info(mt):\n",
    "    # add sample info per variant\n",
    "    mt = mt.annotate_rows(\n",
    "        samples = hl.bind(lambda x: hl.delimit(x, \",\"), hl.agg.filter(mt.GT.n_alt_alleles() > 0, hl.agg.collect(mt.s))),\n",
    "        hetz_samples = hl.bind(lambda x: hl.delimit(x, \",\"), hl.agg.filter(mt.GT.is_het(), hl.agg.collect(mt.s))),\n",
    "        homo_samples = hl.bind(lambda x: hl.delimit(x, \",\"), hl.agg.filter(mt.GT.is_hom_var(), hl.agg.collect(mt.s))),\n",
    "    )\n",
    "    return mt\n",
    "\n",
    "def get_annot_table(chr_num):\n",
    "    # read chromosome file\n",
    "    mt_filtered = get_chrm_mt(chr_num)\n",
    "    # filter for rare variants only\n",
    "    mt_filtered = get_rare_variants(mt_filtered)\n",
    "    # annotate variant qc\n",
    "    mt_filtered = variant_qc(mt_filtered)\n",
    "    # annotate sample qc\n",
    "    mt_filtered = sample_qc(mt_filtered)\n",
    "    # add vep annotations\n",
    "    mt_filtered = add_vep_annotations(mt_filtered)\n",
    "    # add dbnsfp annotations\n",
    "    mt_filtered = add_dbnsfp_annotations(mt_filtered)\n",
    "    # keep deleterious variants\n",
    "    mt_filtered = keep_deleterious_variants(mt_filtered)\n",
    "    # add sample info\n",
    "    mt_filtered = add_sample_info(mt_filtered)\n",
    "    # get burden table\n",
    "    annot_table = mt_filtered.rows()\n",
    "    # save as hail ht\n",
    "    url = save_in_hail_format(annot_table, \"exomes\", f\"chr{chr_num}_annot.ht\", rerun=True)\n",
    "    annot_table = hl.read_table(url)\n",
    "    # reload the table in pandas\n",
    "    annot_df = annot_table.to_pandas()\n",
    "    annot_df[\"alleles\"] = annot_df.alleles.apply(lambda x: \"_\".join(x))\n",
    "    return annot_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "chr_num = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "annot_df = get_annot_table(chr_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "proj_dir = f\"/notebooks/wes/variant_annot/data/\"\n",
    "annot_df_name = f\"chr{chr_num}.tsv.gz\"\n",
    "annot_df.to_csv(annot_df_name, sep='\\t', index=False)\n",
    "upload_file_to_project(annot_df_name, proj_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "hl.stop()\n",
    "spark.sparkContext.stop()\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
