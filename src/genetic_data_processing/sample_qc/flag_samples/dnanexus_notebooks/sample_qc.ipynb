{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import hail as hl\n",
    "import os\n",
    "import time\n",
    "import dxpy\n",
    "import logging\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "\n",
    "# Had to set the configuration to navigate RDD partition error\n",
    "# Build spark\n",
    "builder = (\n",
    "    SparkSession\n",
    "    .builder\n",
    "    .appName(\"Autosome QC\")  # Set a meaningful application name\n",
    "    # .config(\"spark.driver.memory\", \"96g\")  # Set driver memory (e.g., 8 GB)\n",
    "    # .config(\"spark.executor.memory\", \"108g\")  # Set executor memory (e.g., 16 GB)\n",
    "    # .config(\"spark.executor.cores\", \"30\")  # Optional: Set number of cores per executor \n",
    "    .enableHiveSupport()\n",
    ")\n",
    "spark = builder.getOrCreate()\n",
    "\n",
    "hl.init(sc=spark.sparkContext, idempotent=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Define GLOBALS\n",
    "AUTOSOMES_RERUN=False\n",
    "SAMPLE_QC_RERUN=False\n",
    "SAMPLE_QC_RES_RERUN=True\n",
    "SAMPLE_ANNOT_ALL_RERUN=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def save_in_hail_format(hail_obj, db_name, hail_obj_name, rerun):\n",
    "    # Create DB if it does not exist\n",
    "    stmt = f\"CREATE DATABASE IF NOT EXISTS {db_name} LOCATION 'dnax://'\"\n",
    "    spark.sql(stmt).show()\n",
    "    # Find database ID of newly created database using dxpy method\n",
    "    db_uri = dxpy.find_one_data_object(name=f\"{db_name}\".lower(), classname=\"database\")['id']\n",
    "    # Write hail object\n",
    "    url = f\"dnax://{db_uri}/{hail_obj_name}\"\n",
    "    if rerun:\n",
    "        hail_obj.write(url, overwrite=True)\n",
    "    return url\n",
    "\n",
    "def get_url(db_name, hail_obj_name):\n",
    "    # Find database ID of newly created database using dxpy method\n",
    "    db_uri = dxpy.find_one_data_object(name=f\"{db_name}\".lower(), classname=\"database\")['id']\n",
    "    # Write hail object\n",
    "    url = f\"dnax://{db_uri}/{hail_obj_name}\"\n",
    "    return url\n",
    "\n",
    "def get_chrm_mt(chr_num):\n",
    "    db_name = f\"exomes\"\n",
    "    # Find database ID of newly created database using dxpy method\n",
    "    db_uri = dxpy.find_one_data_object(name=f\"{db_name}\".lower(), classname=\"database\")['id']\n",
    "    url = f\"dnax://{db_uri}/chr{chr_num}_vqc.mt\"\n",
    "    mt = hl.read_matrix_table(url)\n",
    "    return mt\n",
    "\n",
    "def upload_file_to_project(filename, proj_dir):\n",
    "    dxpy.upload_local_file(filename, folder=proj_dir, parents=True)\n",
    "    print(f\"*********{filename} uploaded!!*********\")\n",
    "    os.remove(filename)\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "if AUTOSOMES_RERUN:\n",
    "    autosome_mts = [get_chrm_mt(chr_num) for chr_num in range(1,23)]\n",
    "    autosome_mt = hl.MatrixTable.union_rows(*autosome_mts)\n",
    "    url = save_in_hail_format(autosome_mt, \"exomes\", \"autosomes_vqc.mt\", AUTOSOMES_RERUN)\n",
    "    autosome_mt = hl.read_matrix_table(url)\n",
    "else:\n",
    "    url = get_url(\"exomes\", \"autosomes_vqc.mt\")\n",
    "    autosome_mt = hl.read_matrix_table(url)\n",
    "    print(autosome_mt.n_partitions())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "autosome_mt.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "if SAMPLE_QC_RERUN:\n",
    "    autosome_mt = hl.sample_qc(autosome_mt)\n",
    "    sample_qc_ht = autosome_mt.cols()\n",
    "    sample_qc_ht = sample_qc_ht.annotate(\n",
    "        r_snv_indel=((sample_qc_ht.sample_qc.n_snp)/(sample_qc_ht.sample_qc.n_insertion + sample_qc_ht.sample_qc.n_deletion)),\n",
    "    )\n",
    "    url = save_in_hail_format(sample_qc_ht, \"sample_qc\", \"sample_qc_annot.ht\", rerun=SAMPLE_QC_RERUN)\n",
    "    sample_qc_ht = hl.read_table(url)\n",
    "else:\n",
    "    url = get_url(\"sample_qc\", \"sample_qc_annot.ht\")\n",
    "    sample_qc_ht = hl.read_table(url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "sample_qc_ht.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "if SAMPLE_QC_RES_RERUN:\n",
    "    from gnomad.sample_qc.filtering import compute_qc_metrics_residuals\n",
    "    url = get_url(\"sample_qc\", \"sample_annot.ht\")\n",
    "    sample_annot_ht = hl.read_table(url)\n",
    "    \n",
    "    sample_qc_ht = sample_qc_ht.annotate(\n",
    "        pca=sample_annot_ht[sample_qc_ht.s].pca\n",
    "    )\n",
    "    \n",
    "    sample_residuals_ht = compute_qc_metrics_residuals(\n",
    "        sample_qc_ht, pc_scores=sample_qc_ht.pca, qc_metrics={\n",
    "            \"r_ti_tv\": sample_qc_ht.sample_qc.r_ti_tv,\n",
    "            \"r_het_hom_var\": sample_qc_ht.sample_qc.r_het_hom_var,\n",
    "            \"r_insertion_deletion\": sample_qc_ht.sample_qc.r_insertion_deletion,\n",
    "            \"n_singleton\": sample_qc_ht.sample_qc.n_singleton,\n",
    "            \"r_snv_indel\": sample_qc_ht.r_snv_indel,\n",
    "        }\n",
    "    )\n",
    "    url = save_in_hail_format(sample_residuals_ht, \"sample_qc\", \"sample_annot_res.ht\", rerun=SAMPLE_QC_RES_RERUN)\n",
    "    sample_qc_residuals_ht = hl.read_table(url)\n",
    "\n",
    "else:\n",
    "    url = get_url(\"sample_qc\", \"sample_annot_res.ht\")\n",
    "    sample_qc_residuals_ht = hl.read_table(url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final set of sample qc annotations:\n",
    "\n",
    "1. Duplicates\n",
    "2. Related\n",
    "3. Ratio of heterozygous concordance between array and exomes\n",
    "4. Sample call rate\n",
    "5. Eight SD deviation mean ancestry normalized\n",
    "    - Transition/transversion ratio\n",
    "    - Insertion/Deletion allele ratio\n",
    "    - Heterozygous/homozygous call ratio\n",
    "    - SNV/indel \n",
    "    - number of singletons\n",
    "6. Sex from survey\n",
    "7. Genetic sex from array\n",
    "8. Genetic sex from exomes\n",
    "9. Sex chromosome aneuploidy\n",
    "10. Genetic kinship to other participants\n",
    "11. Outlier for heterozygosity or missingness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "if SAMPLE_ANNOT_ALL_RERUN:\n",
    "    url = get_url(\"sample_qc\", \"sample_annot.ht\")\n",
    "    sample_annot_ht = hl.read_table(url)\n",
    "    # add call rate\n",
    "    sample_annot_ht = sample_annot_ht.annotate(\n",
    "            call_rate=sample_qc_ht[sample_annot_ht.s].sample_qc.call_rate\n",
    "        )\n",
    "    # add all residuals\n",
    "    sample_annot_ht = sample_annot_ht.join(sample_qc_residuals_ht)\n",
    "    sample_annot_ht = sample_annot_ht.drop(\"lms\")\n",
    "    url = save_in_hail_format(sample_annot_ht, \"sample_qc\", \"sample_annot_all.ht\", rerun=SAMPLE_ANNOT_ALL_RERUN)\n",
    "    sample_annot_ht = hl.read_table(url)\n",
    "else:\n",
    "    url = get_url(\"sample_qc\", \"sample_annot_all.ht\")\n",
    "    sample_annot_ht = hl.read_table(url)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save to pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def upload_file_to_project(filename, proj_dir):\n",
    "    dxpy.upload_local_file(filename, folder=proj_dir, parents=True)\n",
    "    print(f\"*********{filename} uploaded!!*********\")\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "sample_df = sample_annot_ht.to_pandas()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "proj_dir = f\"/notebooks/wes/sample_qc/data/\"\n",
    "filename = \"sample_qc_annot_all.tsv\"\n",
    "sample_df.to_csv(filename, index=False, sep=\"\\t\")\n",
    "upload_file_to_project(filename, proj_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
