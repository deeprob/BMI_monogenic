{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import hail as hl\n",
    "import os\n",
    "import time\n",
    "import dxpy\n",
    "import logging\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Had to set the configuration to navigate RDD partition error\n",
    "# Build spark\n",
    "builder = (\n",
    "    SparkSession\n",
    "    .builder\n",
    "    .appName(\"HailApplication\")  # Set a meaningful application name\n",
    "    .config(\"spark.driver.memory\", \"12g\")  # Set driver memory (e.g., 8 GB)\n",
    "    .config(\"spark.executor.memory\", \"50g\")  # Set executor memory (e.g., 16 GB)\n",
    "    .config(\"spark.executor.cores\", \"6\")  # Optional: Set number of cores per executor \n",
    "    .enableHiveSupport()\n",
    ")\n",
    "spark = builder.getOrCreate()\n",
    "\n",
    "hl.init(sc=spark.sparkContext, idempotent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Access the Spark context from Hail\n",
    "spark_conf = hl.current_backend()._jbackend.sc().getConf()\n",
    "\n",
    "# Retrieve specific Spark configurations\n",
    "executor_memory = spark_conf.get(\"spark.executor.memory\", \"Not set\")\n",
    "driver_memory = spark_conf.get(\"spark.driver.memory\", \"Not set\")\n",
    "worker_cores = spark_conf.get(\"spark.executor.cores\", \"Not set\")\n",
    "\n",
    "print(f\"Executor Memory: {executor_memory}\")\n",
    "print(f\"Driver Memory: {driver_memory}\")\n",
    "print(f\"Worker Cores: {worker_cores}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Checkpoint 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# checkpoint save\n",
    "# Create database in DNAX\n",
    "db_name = f\"ancestry_inference\"\n",
    "stmt = f\"CREATE DATABASE IF NOT EXISTS {db_name} LOCATION 'dnax://'\"\n",
    "print(stmt)\n",
    "spark.sql(stmt).show()\n",
    "\n",
    "db_uri = dxpy.find_one_data_object(name=f\"{db_name}\".lower(), classname=\"database\")['id']\n",
    "mt_name = f\"geno_overlap_filtered.mt\"\n",
    "url = f\"dnax://{db_uri}/{mt_name}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "RERUN=False\n",
    "if RERUN:\n",
    "    geno_mt.write(url, overwrite=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "geno_mt = hl.read_matrix_table(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "geno_mt.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save PCA projections for geno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# checkpoint save\n",
    "# Create database in DNAX\n",
    "db_name = f\"ancestry_inference\"\n",
    "stmt = f\"CREATE DATABASE IF NOT EXISTS {db_name} LOCATION 'dnax://'\"\n",
    "print(stmt)\n",
    "spark.sql(stmt).show()\n",
    "\n",
    "db_uri = dxpy.find_one_data_object(name=f\"{db_name}\".lower(), classname=\"database\")['id']\n",
    "mt_name = f\"pca_loadings.ht\"\n",
    "url = f\"dnax://{db_uri}/{mt_name}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "RERUN=False\n",
    "if RERUN:\n",
    "    loadings_ht.write(url, overwrite=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "loadings_ht = hl.read_table(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "loadings_ht.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Project new genotypes onto loadings\n",
    "\n",
    "ht = hl.experimental.pc_project(geno_mt.GT, loadings_ht.loadings, loadings_ht.af)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# checkpoint save\n",
    "# Create database in DNAX\n",
    "db_name = f\"ancestry_inference\"\n",
    "stmt = f\"CREATE DATABASE IF NOT EXISTS {db_name} LOCATION 'dnax://'\"\n",
    "print(stmt)\n",
    "spark.sql(stmt).show()\n",
    "\n",
    "db_uri = dxpy.find_one_data_object(name=f\"{db_name}\".lower(), classname=\"database\")['id']\n",
    "mt_name = f\"geno_sample_pca.ht\"\n",
    "url = f\"dnax://{db_uri}/{mt_name}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "RERUN=True\n",
    "if RERUN:\n",
    "    ht.write(url, overwrite=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "ht = hl.read_table(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "ht.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "geno_pca_df = ht.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "geno_pca_df[[f\"pca_{i}\" for i in range(1, 21)]] = pd.DataFrame(geno_pca_df.scores.tolist(), index= geno_pca_df.index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "geno_pca_df = geno_pca_df.drop(columns=[\"scores\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def upload_file_to_project(filename, proj_dir):\n",
    "    dxpy.upload_local_file(filename, folder=proj_dir, parents=True)\n",
    "    print(f\"*********{filename} uploaded!!*********\")\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "proj_dir = f\"/notebooks/ancestry_inference/data/\"\n",
    "filename = \"geno_pca.csv.gz\"\n",
    "geno_pca_df.to_csv(filename, index=False)\n",
    "upload_file_to_project(filename, proj_dir)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "geno_pca_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
