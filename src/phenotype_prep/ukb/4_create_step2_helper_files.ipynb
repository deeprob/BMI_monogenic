{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"trusted":true},"outputs":[],"source":["import os \n","import dxpy\n","import pandas as pd"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"trusted":true},"outputs":[],"source":["RERUN=False"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"trusted":true},"outputs":[],"source":["def annotate_variant_consequence(ser):\n","    annot = pd.NA\n","    consequence =  set(ser.consequence.split(\";\"))\n","    ptv_terms = set([\"frameshift_variant\", \"stop_gained\", \"splice_acceptor_variant\", \"splice_donor_variant\"])\n","    if len(ptv_terms.intersection(consequence))>0:\n","        annot = \"lof\"\n","    elif \"missense_variant\" in consequence:\n","        if ser.del_score==9:\n","            annot = \"missense_strict\"\n","        elif ser.del_score>6:\n","            annot = \"missense_lenient\"\n","    return annot\n","\n","def keep_most_del(vals):\n","    vals = set(vals)\n","    if \"lof\" in vals:\n","        return \"lof\"\n","    elif \"missense_strict\" in vals:\n","        return \"missense_strict\"\n","    return list(vals)[0]\n","\n","def create_helper_files(chr_exome_file):\n","    df = pd.read_csv(chr_exome_file, sep=\"\\t\")\n","    # get the variants in correct format\n","    df[\"variants\"] = df.locus.str.lstrip(\"chr\") + \":\" + df.alleles.str.replace(\"_\", \":\")\n","    # add lof and missense annotations\n","    df[\"annotation\"] = df.apply(annotate_variant_consequence, axis=1)\n","    # filter by annotations, biotype, and call rate\n","    df = df.loc[(df.annotation.isin([\"lof\", \"missense_strict\", \"missense_lenient\"]))&(df.biotype==\"protein_coding\")&(df.call_rate>0.5)]\n","    return df\n","    \n","\n","def upload_file_to_project(filename, proj_dir):\n","    dxpy.upload_local_file(filename, folder=proj_dir, parents=True)\n","    print(f\"*********{filename} uploaded!!*********\")\n","    os.remove(filename)\n","    return"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"trusted":true},"outputs":[],"source":["if RERUN:\n","    exome_annot_path = \"/mnt/project/notebooks/exome_annot/\"\n","\n","    vcfs_per_chrm = {\n","        \"chr1\": 97, \"chr2\": 71, \"chr3\": 56, \"chr4\": 39, \"chr5\": 43, \"chr6\": 48, \n","        \"chr7\": 47, \"chr8\": 35, \"chr9\": 42, \"chr10\": 40, \"chr11\": 57, \"chr12\": 52, \n","        \"chr13\": 18, \"chr14\": 30, \"chr15\": 34, \"chr16\": 47, \"chr17\": 56, \"chr18\": 16, \n","        \"chr19\": 65, \"chr20\": 25, \"chr21\": 11, \"chr22\": 23, \"chrX\": 24,\n","    }\n","\n","    burden_dfs = []\n","    # pool = mp.Pool(mp.cpu_count()-1)\n","\n","    def get_burden_dfs(chrm):\n","        burden_dfs_chrm = []\n","        chr_exome_files = [os.path.join(exome_annot_path, chrm, \"annot_tables_vep109_v4\", f\"block_{i}.tsv.gz\") for i in range(vcfs_per_chrm[chrm])]\n","        for ef in chr_exome_files:\n","            burden_df = create_helper_files(ef)\n","            burden_dfs_chrm.append(burden_df)\n","        return burden_dfs_chrm\n","\n","    chrms = [f\"chr{i}\" for i in list(range(1,23))] +[\"chrX\"]\n","    for chrm in chrms:\n","        print(chrm,  end=\" \")\n","        burden_dfs_chrm = get_burden_dfs(chrm)\n","        burden_dfs.extend(burden_dfs_chrm)\n","\n","    burden_df = pd.concat(burden_dfs)\n","    burden_df.to_csv(\"gene_burden.csv.gz\")\n","    upload_file_to_project(\"gene_burden.csv.gz\", \"/notebooks/regenie/data/\")\n","    \n","else:\n","    burden_df=pd.read_csv(\"/mnt/project/notebooks/regenie/data/gene_burden.csv.gz\")"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"trusted":true},"outputs":[],"source":["gene_burden_df=burden_df.copy()\n","pheno_df= pd.read_csv(\"/mnt/project/notebooks/regenie/data/british_phenotype.tsv.gz\", sep=\"\\t\")"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"trusted":true},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"trusted":true},"outputs":[],"source":["def create_helper_files(all_burden_df, terms, pheno_df, lf):\n","    terms = terms.split(\",\")\n","    burden_df = all_burden_df.loc[all_burden_df.annotation.isin(terms)]\n","    discovery_samples = set(pheno_df.IID.astype(str))\n","    if not lf:\n","        # filter by number of samples in the discovery cohort\n","        gene_sample_df = burden_df.groupby(\"gene\").agg({\"samples\": lambda x: set(\",\".join(x).split(\",\"))})\n","        gene_sample_df[\"samples\"] = gene_sample_df.samples.apply(lambda x: x.intersection(discovery_samples))\n","        min_sample_genes = set(gene_sample_df.loc[gene_sample_df.samples.apply(lambda x: len(x))>=30].index)\n","        burden_df = burden_df.loc[burden_df.gene.isin(min_sample_genes)]\n","    else:\n","        # filter by number of samples carrying genes and lifestyle factors in the discovery cohort\n","        lf_samples = set(pheno_df.loc[pheno_df[lf]==1, \"IID\"].astype(str))\n","        gene_sample_df = burden_df.groupby(\"gene\").agg({\"samples\": lambda x: set(\",\".join(x).split(\",\"))})\n","        gene_sample_df[\"samples\"] = gene_sample_df.samples.apply(lambda x: x.intersection(lf_samples).intersection(discovery_samples))\n","        min_sample_genes = set(gene_sample_df.loc[gene_sample_df.samples.apply(lambda x: len(x))>=10].index)\n","        burden_df = burden_df.loc[burden_df.gene.isin(min_sample_genes)]\n","    \n","    # create annotation df\n","    annot_df = burden_df.loc[:, [\"variants\", \"gene\", \"annotation\"]]\n","    # this gets rid of duplicates due to transcripts in same gene with same consequence\n","    annot_df = annot_df.dropna().drop_duplicates()\n","    # this annotates the same locus for the same gene with the most severe consequence\n","    annot_df =  annot_df.groupby([\"variants\", \"gene\"]).agg({\"annotation\": lambda x: keep_most_del(x)}).reset_index()\n","    # create set list df\n","    set_df = annot_df.groupby(\"gene\").agg({\"variants\": lambda x: \",\".join(x)})\n","    set_df[[\"chrm\", \"location\"]] = set_df.variants.apply(lambda x: pd.Series(dict(zip([\"chrm\", \"location\"], x.split(\",\")[0].split(\":\")[:2]))))\n","    set_df = set_df.reset_index().loc[:, [\"gene\", \"chrm\", \"location\", \"variants\"]]\n","    # create aaf df\n","    aaf_df = burden_df.loc[:, [\"variants\", \"maf\"]]\n","    aaf_df = aaf_df.dropna().drop_duplicates()\n","    return annot_df, set_df, aaf_df\n","    "]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"jupyter":{"outputs_hidden":true},"tags":[],"trusted":true},"outputs":[],"source":["mask_names = [\"PTV\", \"PTV_Missense_strict\", \"PTV_Missense_lenient\"]\n","categories =  [\"lof\", \"lof,missense_strict\", \"lof,missense_strict,missense_lenient\"]\n","lfs = [\"\", \"pa\", \"alcohol\", \"smoke\", \"sleep\", \"sedentary\", \"diet\"]\n","\n","\n","for mask, cat in zip(mask_names, categories):\n","    print(cat)\n","    for lf in lfs:\n","        print(lf)\n","        print(mask)\n","        annot_df, set_df, aaf_df = create_helper_files(gene_burden_df, cat, pheno_df, lf)\n","        mask_df = pd.DataFrame({\"mask_name\": [mask], \"categories\": [cat]})\n","        annot_df_name = \"ukb_annotations.tsv.gz\"\n","        set_df_name = \"ukb_sets.tsv.gz\"\n","        aaf_df_name = \"ukb_aafs.tsv.gz\"\n","        mask_df_name = \"ukb_masks.tsv.gz\"\n","        proj_dir = f\"/notebooks/regenie/data/step2/{mask}/{lf}/\"\n","        for df, name in zip(\n","            [annot_df, set_df, aaf_df, mask_df],\n","            [annot_df_name, set_df_name, aaf_df_name, mask_df_name]\n","        ):\n","            df.to_csv(name, sep='\\t', index=False, header=False)\n","            # upload table to project\n","            upload_file_to_project(name, proj_dir)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.16"}},"nbformat":4,"nbformat_minor":4}
