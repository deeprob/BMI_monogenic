{"cells":[{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from pyspark.sql import SparkSession\n","import hail as hl\n","import os\n","import time\n","import dxpy\n","import logging\n","import pandas as pd\n","import re\n","\n","\n","# Build spark\n","builder = (\n","    SparkSession\n","    .builder\n","    .enableHiveSupport()\n",")\n","spark = builder.getOrCreate()\n","hl.init(sc=spark.sparkContext, idempotent=True)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def get_rare_variants(mt):\n","    \"\"\"\n","    Returns a matrix table with alt allele frequency < 0.01\n","    \"\"\"\n","    mt = mt.annotate_rows(gt_stats = hl.agg.call_stats(mt.GT, mt.alleles))\n","    # filter to keep rare variants and variants present in at least one sample\n","    mt = mt.filter_rows((mt.gt_stats.AF[1] < 0.01) & (mt.gt_stats.AC[1] > 0))\n","    # add maf and mac info\n","    mt = mt.annotate_rows(maf=mt.gt_stats.AF[1], mac=mt.gt_stats.AC[1])\n","    return mt\n","\n","\n","def variant_qc(mt):\n","    mt = hl.variant_qc(mt)\n","    # annotate variant call rate, hwe p-value, mean read depth\n","    mt = mt.annotate_rows(\n","        call_rate=mt.variant_qc.call_rate,\n","        p_value_hwe=mt.variant_qc.p_value_hwe,\n","        mean_rd=mt.variant_qc.dp_stats.min,\n","    )\n","    return mt\n","\n","\n","def add_vep_annotations(mt, vep_file=\"file:///mnt/project/exome_annot/annot_run/vep_config_109_v7.json\"):\n","    \"\"\"\n","    Add vep and dbnsfp annotations\n","    \"\"\"\n","    # add vep annotations\n","    mt = hl.vep(mt, vep_file) # annot table with vep\n","    # combine multiple consequences for a single transcript into one string\n","    mt = mt.annotate_rows(consequences = hl.map(lambda x: hl.delimit(x, delimiter=\";\"), mt.vep.transcript_consequences.consequence_terms))\n","    # annotate genes, transcripts, consequences and biotype\n","    mt = mt.annotate_rows(gene_transcript_consequence_biotype = hl.zip(\n","        mt.vep.transcript_consequences.gene_symbol,\n","        mt.vep.transcript_consequences.transcript_id,\n","        mt.consequences,\n","        mt.vep.transcript_consequences.biotype,\n","        mt.vep.transcript_consequences.lof,\n","    ))\n","    # only keep relevant columns\n","    mt = mt.select_rows(\n","        mt.gene_transcript_consequence_biotype,\n","        mt.maf, mt.mac,\n","        mt.call_rate, mt.p_value_hwe, mt.mean_rd\n","    )\n","    # explode by gene-trancsript-consequence column\n","    mt = mt.explode_rows(\"gene_transcript_consequence_biotype\")\n","    \n","    # get plof and missense mutations\n","    lof_mutations = \"stop_gained|frameshift_variant|stop_lost|start_lost\"\n","    splice_lof_mutations = \"splice_acceptor_variant|splice_donor_variant\"\n","    missense_mutations = \"missense_variant\"\n","    \n","    mt = mt.annotate_rows(\n","        lof = mt.gene_transcript_consequence_biotype[2].matches(lof_mutations),\n","        missense = mt.gene_transcript_consequence_biotype[2].matches(missense_mutations),\n","        splice_lof = mt.gene_transcript_consequence_biotype[2].matches(splice_lof_mutations)\n","    )\n","    \n","    # filter for these mutation types\n","    mt = mt.filter_rows((mt.lof==True)|(mt.splice_lof==True)|(mt.missense==True))\n","    return mt\n","\n","\n","def create_deleteriousness_scores(mt):\n","    metrics = [\"SIFT\", \"LRT\", \"FATHMM\", \"PROVEAN\", \"MetaSVM\", \"MetaLR\", \"PrimateAI\", \"DEOGEN2\", \"MutationAssessor\"]\n","    kwd_dict = {f\"{m}_pred\": hl.dict(hl.zip(\n","            mt.dbNSFP_variants.genename,\n","            hl.map(lambda x: hl.dict(hl.zip(x[0].split(\";\"), x[1].split(\";\"))), hl.zip(mt.dbNSFP_variants.Ensembl_transcriptid, mt.dbNSFP_variants[f\"{m}_pred\"]))\n","    )) for m in metrics}\n","    mt = mt.annotate_rows(**kwd_dict)\n","    \n","    def get_del_score_func(gtcb, del_pred):\n","        gene = gtcb[0]\n","        transcript = gtcb[1]\n","        val = hl.if_else(del_pred.contains(gene) & del_pred[gene].contains(transcript) & (del_pred[gene][transcript]==\"D\"), 1, 0)\n","        return val\n","    \n","    kwd_dict = {f\"{m}_pred\": get_del_score_func(mt.gene_transcript_consequence_biotype, mt[f\"{m}_pred\"]) for m in metrics[:-1]}\n","    mt = mt.annotate_rows(**kwd_dict)\n","\n","    def get_del_score_func_ma(gtcb, del_pred):\n","        gene = gtcb[0]\n","        transcript = gtcb[1]\n","        val = hl.if_else(del_pred.contains(gene) & del_pred[gene].contains(transcript) & (del_pred[gene][transcript]==\"H\"), 1, 0)\n","        return val\n","    \n","    mt = mt.annotate_rows(MutationAssessor_pred = get_del_score_func_ma(mt.gene_transcript_consequence_biotype, mt.MutationAssessor_pred))\n","    cols2sum = [f\"{m}_pred\" for m in metrics]\n","    mt = mt.annotate_rows(del_score=hl.sum([mt[col] for col in cols2sum]))\n","    return mt\n","    \n","\n","def add_dbnsfp_annotations(mt):\n","    db = hl.experimental.DB(region='us', cloud='aws')\n","    mt = db.annotate_rows_db(mt, 'dbNSFP_variants') # add dbNSFP annotations\n","    mt = create_deleteriousness_scores(mt)\n","    return mt\n","\n","\n","def keep_deleterious_variants(mt):\n","    # filter to keep deleterious mutations only\n","    mt = mt.filter_rows((mt.lof==True)|(mt.splice_lof==True)|((mt.missense==True)&(mt.del_score>4)))\n","    # annotate properly\n","    mt = mt.annotate_rows(\n","        gene = mt.gene_transcript_consequence_biotype[0],\n","        transcript = mt.gene_transcript_consequence_biotype[1],\n","        consequence = mt.gene_transcript_consequence_biotype[2],\n","        biotype = mt.gene_transcript_consequence_biotype[3],\n","        loftee = mt.gene_transcript_consequence_biotype[4]\n","    )\n","    # only keep relevant columns\n","    mt  = mt.select_rows(\n","        mt.gene, mt.transcript, mt.consequence, mt.biotype, mt.loftee,\n","        mt.lof, mt.splice_lof, mt.missense, mt.del_score,\n","        mt.maf, mt.mac, mt.call_rate, mt.p_value_hwe, mt.mean_rd\n","    )\n","    return mt\n","\n","\n","def add_sample_info(mt):\n","    # add sample info per variant\n","    mt = mt.annotate_rows(\n","        samples = hl.bind(lambda x: hl.delimit(x, \",\"), hl.agg.filter(mt.GT.n_alt_alleles() > 0, hl.agg.collect(mt.s))),\n","        hetz_samples = hl.bind(lambda x: hl.delimit(x, \",\"), hl.agg.filter(mt.GT.is_het(), hl.agg.collect(mt.s))),\n","        homo_samples = hl.bind(lambda x: hl.delimit(x, \",\"), hl.agg.filter(mt.GT.is_hom_var(), hl.agg.collect(mt.s))),\n","    )\n","    return mt\n","\n","\n","def get_annot_table(mt):\n","    # split multi-allelic hits to bi-allelic\n","    mt_filtered = hl.split_multi_hts(mt, permit_shuffle=True)\n","    # variant qc\n","    mt_filtered = variant_qc(mt_filtered)\n","    # filter for rare variants only\n","    mt_filtered = get_rare_variants(mt_filtered)\n","    # add vep annotations\n","    mt_filtered = add_vep_annotations(mt_filtered)\n","    # add dbnsfp annotations\n","    mt_filtered = add_dbnsfp_annotations(mt_filtered)\n","    # keep deleterious variants\n","    mt_filtered = keep_deleterious_variants(mt_filtered)\n","    # add sample info\n","    mt_filtered = add_sample_info(mt_filtered)\n","    # get burden table\n","    annot_table = mt_filtered.rows()\n","    annot_df = annot_table.to_pandas()\n","    annot_df[\"alleles\"] = annot_df.alleles.apply(lambda x: \"_\".join(x))\n","    return annot_df\n","\n","\n","def upload_file_to_project(filename, proj_dir):\n","    dxpy.upload_local_file(filename, folder=proj_dir, parents=True)\n","    print(f\"*********{filename} uploaded!!*********\")\n","    os.remove(filename)\n","    return"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["vcf_dir = \"/mnt/project/Bulk/Exome sequences/Population level exome OQFE variants, pVCF format - final release/\"\n","chr_num = \"1\"\n","vcf_files = sorted([\"file://\" + os.path.join(vcf_dir, fp) for fp in os.listdir(vcf_dir) if (f\"_c{chr_num}_\" in fp and fp.endswith(\"vcf.gz\"))])\n","\n","# Annotation configure\n","logging.basicConfig(filename=f\"chr{chr_num}_annot_vep109_v4.log\", level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s', datefmt='%m/%d/%Y %I:%M:%S %p')\n","\n","i=0\n","\n","# change i to new value if the instance restarts\n","proj_dir = f\"/mnt/project/exome_annot/annot_run/notebooks/chr{chr_num}/annot_tables_vep109_v4/\"\n","\n","if os.path.exists(proj_dir):\n","    existing_files = sorted([fp for fp in os.listdir(proj_dir)], key=lambda x: int(''.join(filter(str.isdigit, x))))\n","    last_file = existing_files[-1]\n","    pattern = re.compile(\"^block_(\\d+).tsv.gz$\")\n","    m = re.match(pattern, last_file)\n","    i = int(m.groups()[0])\n","    \n","while i<len(vcf_files):\n","    time_start = time.time()\n","    \n","    # read the matrix table\n","    db_name = f\"exome_chr{chr_num}\"\n","    db_uri = dxpy.find_one_data_object(name=f\"{db_name}\".lower(), classname=\"database\")['id']\n","    mt_name = f\"block_{i}.mt\"\n","    mt_url = f\"dnax://{db_uri}/{mt_name}\"\n","    mt = hl.read_matrix_table(mt_url)\n","    \n","    try:\n","        # create annot table\n","        annot_df = get_annot_table(mt)\n","        # save annot table to local\n","        annot_df_name = f\"block_{i}.tsv.gz\"\n","        annot_df.to_csv(annot_df_name, sep='\\t', index=False)\n","        # upload table to project\n","        proj_dir = f\"/exome_annot/annot_run/notebooks/chr{chr_num}/annot_tables_vep109_v4/\"\n","        upload_file_to_project(annot_df_name, proj_dir)\n","\n","        time_end = time.time()\n","        time_taken = (time_end - time_start)/60\n","        logging.info(f\"Time to annotate block {i}: {time_taken} mins\\n\")\n","\n","        # remove tmp files created by hail to prevent storage issues \n","        tmp_dir = \"/tmp/\"\n","        for file in os.listdir(tmp_dir):\n","            if file.startswith(\"persist_Table\"):\n","                os.remove(os.path.join(tmp_dir, file))\n","                \n","    except Exception as error:\n","        logging.warning(f\"block {i} not annotated due to {error}\\n\")\n","        print(f\"!!!!!!!!block {i} not annotated!!!!!!!!\")\n","        \n","    i+=1"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["hl.stop()\n","spark.sparkContext.stop()\n","spark.stop()"]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.16"}},"nbformat":4,"nbformat_minor":4}
